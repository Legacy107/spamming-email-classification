{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:20:10.992008Z","iopub.status.busy":"2023-10-02T10:20:10.991641Z","iopub.status.idle":"2023-10-02T10:21:09.409210Z","shell.execute_reply":"2023-10-02T10:21:09.407921Z","shell.execute_reply.started":"2023-10-02T10:20:10.991960Z"},"trusted":true},"outputs":[],"source":["from pathlib import Path\n","path = Path('../input/spamham-email-classification-nlp')\n","! pip install -q -U datasets \n","! pip install -q -U evaluate\n","! pip install -q -U huggingface_hub\n","! pip install -q -U peft\n","! pip install -q -U transformers"]},{"cell_type":"markdown","metadata":{},"source":["**Optional:** Login with huggingface to save dataset and model. Huggingface hub is a repository hub similar to Github but for ML. You can explore more [here](https://huggingface.co/)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:21:09.412037Z","iopub.status.busy":"2023-10-02T10:21:09.411688Z","iopub.status.idle":"2023-10-02T10:21:09.652616Z","shell.execute_reply":"2023-10-02T10:21:09.651776Z","shell.execute_reply.started":"2023-10-02T10:21:09.411999Z"},"trusted":true},"outputs":[],"source":["from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"markdown","metadata":{},"source":["The data is in the emails.csv file. For opening, manipulating, and viewing CSV files, it's generally best to use the Pandas library"]},{"cell_type":"markdown","metadata":{},"source":["## Load and process data"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:39:49.884873Z","iopub.status.busy":"2023-10-02T09:39:49.884513Z","iopub.status.idle":"2023-10-02T09:39:50.443553Z","shell.execute_reply":"2023-10-02T09:39:50.442425Z","shell.execute_reply.started":"2023-10-02T09:39:49.884843Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","df = pd.read_csv(path/'emails.csv')"]},{"cell_type":"markdown","metadata":{},"source":["This creates a DataFrame, which is a table of named columns, a bit like a database table. To view the first and last rows, and row count of a DataFrame, just type its name:"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:39:50.446257Z","iopub.status.busy":"2023-10-02T09:39:50.445609Z","iopub.status.idle":"2023-10-02T09:39:50.467140Z","shell.execute_reply":"2023-10-02T09:39:50.465987Z","shell.execute_reply.started":"2023-10-02T09:39:50.446215Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Text</th>\n","      <th>Spam</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Subject: naturally irresistible your corporate...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Subject: the stock trading gunslinger  fanny i...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Subject: unbelievable new homes made easy  im ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Subject: 4 color printing special  request add...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Subject: do not have money , get software cds ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5723</th>\n","      <td>Subject: re : research and development charges...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5724</th>\n","      <td>Subject: re : receipts from visit  jim ,  than...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5725</th>\n","      <td>Subject: re : enron case study update  wow ! a...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5726</th>\n","      <td>Subject: re : interest  david ,  please , call...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5727</th>\n","      <td>Subject: news : aurora 5 . 2 update  aurora ve...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5728 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["                                                   Text  Spam\n","0     Subject: naturally irresistible your corporate...     1\n","1     Subject: the stock trading gunslinger  fanny i...     1\n","2     Subject: unbelievable new homes made easy  im ...     1\n","3     Subject: 4 color printing special  request add...     1\n","4     Subject: do not have money , get software cds ...     1\n","...                                                 ...   ...\n","5723  Subject: re : research and development charges...     0\n","5724  Subject: re : receipts from visit  jim ,  than...     0\n","5725  Subject: re : enron case study update  wow ! a...     0\n","5726  Subject: re : interest  david ,  please , call...     0\n","5727  Subject: news : aurora 5 . 2 update  aurora ve...     0\n","\n","[5728 rows x 2 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"markdown","metadata":{},"source":["The DataFrame has a useful method called describe() that provides insights on the dataframe."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:39:50.469178Z","iopub.status.busy":"2023-10-02T09:39:50.468783Z","iopub.status.idle":"2023-10-02T09:39:50.500166Z","shell.execute_reply":"2023-10-02T09:39:50.499039Z","shell.execute_reply.started":"2023-10-02T09:39:50.469143Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>5728</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>5695</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>Subject: re : contact info  glenn ,  please , ...</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                     Text\n","count                                                5728\n","unique                                               5695\n","top     Subject: re : contact info  glenn ,  please , ...\n","freq                                                    2"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df.describe(include='object')"]},{"cell_type":"markdown","metadata":{},"source":["We can see that in the 5728 rows, there are only 5695 unique text value. This means the data probably contains some duplicated rows that could create some level of bias. Luckily this is a common problem and pandas provide a method to deal with duplicated values."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:39:50.503882Z","iopub.status.busy":"2023-10-02T09:39:50.503216Z","iopub.status.idle":"2023-10-02T09:39:50.549408Z","shell.execute_reply":"2023-10-02T09:39:50.548265Z","shell.execute_reply.started":"2023-10-02T09:39:50.503843Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>5695</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>5695</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>Subject: naturally irresistible your corporate...</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                     Text\n","count                                                5695\n","unique                                               5695\n","top     Subject: naturally irresistible your corporate...\n","freq                                                    1"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df = df.drop_duplicates()\n","df.describe(include='object')"]},{"cell_type":"markdown","metadata":{},"source":["Now that the data has been cleaned, we need to split the data into 3 sets: train, test and validation. This [blog](https://blog.roboflow.com/train-test-split/) provide excellent explaination on why we need 3 separate sets. In short:\n","* train: this set is used for training your model.\n","* validation: this set is also used during the training process to check for overfitting/underfitting and validate the model hyperparameters.\n","* test: this set acts as a future dataset. It is held out from the model during training and it is only used at the very end of the process for evaluating your model."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:39:50.551941Z","iopub.status.busy":"2023-10-02T09:39:50.551161Z","iopub.status.idle":"2023-10-02T09:39:51.143102Z","shell.execute_reply":"2023-10-02T09:39:51.141748Z","shell.execute_reply.started":"2023-10-02T09:39:50.551903Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['Spam'])\n","val_df, test_df = train_test_split(val_df, test_size=0.5, stratify=val_df['Spam'])"]},{"cell_type":"markdown","metadata":{},"source":["For this demo, the ratio for train/validation/test is 80%/10%/10% respectively. Note that a stratify option is set on column \"Spam\" to ensure that the proportions between spam and \"ham\" emails are the same for all sets. This can be verified after splitting the data"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:39:51.145180Z","iopub.status.busy":"2023-10-02T09:39:51.144780Z","iopub.status.idle":"2023-10-02T09:39:51.155449Z","shell.execute_reply":"2023-10-02T09:39:51.154202Z","shell.execute_reply.started":"2023-10-02T09:39:51.145141Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Spam percentage in train: 0.24012291483757683\n","Spam percentage in validation: 0.24077328646748683\n","Spam percentage in test: 0.24035087719298245\n"]}],"source":["print(\"Spam percentage in train:\", len(train_df[train_df[\"Spam\"] == 1]) / len(train_df))\n","print(\"Spam percentage in validation:\", len(val_df[val_df[\"Spam\"] == 1]) / len(val_df))\n","print(\"Spam percentage in test:\", len(test_df[test_df[\"Spam\"] == 1]) / len(test_df))"]},{"cell_type":"markdown","metadata":{},"source":["The dataframe is then converted to a `Dataset` format, which is more convinient and more efficient to process and use in training."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:39:51.158476Z","iopub.status.busy":"2023-10-02T09:39:51.158088Z","iopub.status.idle":"2023-10-02T09:39:51.164286Z","shell.execute_reply":"2023-10-02T09:39:51.163221Z","shell.execute_reply.started":"2023-10-02T09:39:51.158441Z"},"trusted":true},"outputs":[],"source":["train_df.reset_index(drop=True, inplace=True)\n","val_df.reset_index(drop=True, inplace=True)\n","test_df.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:39:51.166262Z","iopub.status.busy":"2023-10-02T09:39:51.165549Z","iopub.status.idle":"2023-10-02T09:39:51.847896Z","shell.execute_reply":"2023-10-02T09:39:51.846860Z","shell.execute_reply.started":"2023-10-02T09:39:51.166227Z"},"trusted":true},"outputs":[],"source":["from datasets import Dataset,DatasetDict\n","\n","ds = DatasetDict({\n","    \"train\": Dataset.from_pandas(train_df),\n","    \"val\": Dataset.from_pandas(val_df),\n","    \"test\": Dataset.from_pandas(test_df),\n","})"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:39:51.849918Z","iopub.status.busy":"2023-10-02T09:39:51.849341Z","iopub.status.idle":"2023-10-02T09:39:51.857274Z","shell.execute_reply":"2023-10-02T09:39:51.856239Z","shell.execute_reply.started":"2023-10-02T09:39:51.849878Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['Text', 'Spam'],\n","        num_rows: 4556\n","    })\n","    val: Dataset({\n","        features: ['Text', 'Spam'],\n","        num_rows: 569\n","    })\n","    test: Dataset({\n","        features: ['Text', 'Spam'],\n","        num_rows: 570\n","    })\n","})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["ds"]},{"cell_type":"markdown","metadata":{},"source":["**Optional:** Push the dataset to huggingface to persist the data"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:39:51.862906Z","iopub.status.busy":"2023-10-02T09:39:51.862243Z","iopub.status.idle":"2023-10-02T09:39:55.767907Z","shell.execute_reply":"2023-10-02T09:39:55.766637Z","shell.execute_reply.started":"2023-10-02T09:39:51.862868Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b51fd5d3b9764c479a44e2c11c71d4af","version_major":2,"version_minor":0},"text/plain":["Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"066618ec317a460cb343630680c96d74","version_major":2,"version_minor":0},"text/plain":["Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bfa1ccca5a4e4dc9a7e260a264ca2754","version_major":2,"version_minor":0},"text/plain":["Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"346cb01f9e194eb59c6175391abfe56d","version_major":2,"version_minor":0},"text/plain":["Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"717220d9f0bf413b9fb18c36898a9a5e","version_major":2,"version_minor":0},"text/plain":["Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b80cd3965f3b409eb977c0f6a229e463","version_major":2,"version_minor":0},"text/plain":["Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e087e3f6c6c840e19a60774a38960357","version_major":2,"version_minor":0},"text/plain":["Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"af6f1bd03e0a493b95c24113110a2044","version_major":2,"version_minor":0},"text/plain":["Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4115961f314a4351812c8ea2e7dfc56d","version_major":2,"version_minor":0},"text/plain":["Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e3f54a589609461f869a2a92efcbfa2d","version_major":2,"version_minor":0},"text/plain":["Downloading metadata:   0%|          | 0.00/680 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["ds.push_to_hub('spamming-email-classification')"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocessing data for finetuning"]},{"cell_type":"markdown","metadata":{},"source":["**Optional:** Load the dataset from huggingface hub"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:21:09.654788Z","iopub.status.busy":"2023-10-02T10:21:09.654168Z","iopub.status.idle":"2023-10-02T10:21:15.480318Z","shell.execute_reply":"2023-10-02T10:21:15.479245Z","shell.execute_reply.started":"2023-10-02T10:21:09.654755Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"490b9c08d14241c2908fd324083d55f7","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/680 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0ac94af532f414eb074df2c68645c13","version_major":2,"version_minor":0},"text/plain":["Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a0ba754f3eee4942815a01e15d6c1725","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/4.00M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db1a0e85b66b47a48a47cbeb5d502910","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/457k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6ea68aeacd4a43c190483011d4a8ac53","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/507k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0aea963f63849dba8bf9f4ed4bd84a7","version_major":2,"version_minor":0},"text/plain":["Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"773224291f4d4eb5bb692e3c761d1009","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/4556 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aa1399b747d745788673d64ce4bdce68","version_major":2,"version_minor":0},"text/plain":["Generating val split:   0%|          | 0/569 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3e805d7ee49846cc8e4f1885a683094f","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/570 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","ds = load_dataset('legacy107/spamming-email-classification')"]},{"cell_type":"markdown","metadata":{},"source":["Although it is called \"Language model\", the model itself does not receive actual text as its inputs. A deep learning model expects numbers as inputs. So we need to do two things:\n","\n","- *Tokenization*: Split each text up into words (or actually, as we'll see, into *tokens*)\n","- *Numericalization*: Convert each word (or token) into a number.\n","\n","The details about how this is done actually depend on the particular model we use. So first we'll need to pick a model. There are thousands of models available, but a reasonable starting point for nearly any NLP problem is to use the well-knowned `BERT`  model (Bidirectional Encoder Representations from Transformers). This model has a maximum token input length of 512. Note that for this demo, we will use the uncased version which will treat `A` and `a` as the same character.\n","\n","***Note:*** *replace \"base\" with \"large\" for a slower but more accurate model, once you've finished exploring*"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:21:15.483741Z","iopub.status.busy":"2023-10-02T10:21:15.482632Z","iopub.status.idle":"2023-10-02T10:21:15.488222Z","shell.execute_reply":"2023-10-02T10:21:15.487264Z","shell.execute_reply.started":"2023-10-02T10:21:15.483706Z"},"trusted":true},"outputs":[],"source":["model_nm = 'bert-base-uncased'\n","max_length = 512"]},{"cell_type":"markdown","metadata":{},"source":["`AutoTokenizer` will create a tokenizer appropriate for a given model:"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:21:15.490529Z","iopub.status.busy":"2023-10-02T10:21:15.489789Z","iopub.status.idle":"2023-10-02T10:21:18.409024Z","shell.execute_reply":"2023-10-02T10:21:18.408085Z","shell.execute_reply.started":"2023-10-02T10:21:15.490487Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1047e95ac6cd48ae9b668620e20b33d1","version_major":2,"version_minor":0},"text/plain":["Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d3e9ef71af9d461f9172c4c5b7662b66","version_major":2,"version_minor":0},"text/plain":["Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cf37798abdf340da80fc458b7ce088e5","version_major":2,"version_minor":0},"text/plain":["Downloading (â€¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7644686d469244b99fb3aa83bc1e261b","version_major":2,"version_minor":0},"text/plain":["Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","tokz = AutoTokenizer.from_pretrained(model_nm, model_max_length=max_length)"]},{"cell_type":"markdown","metadata":{},"source":["Here's an example of how the tokenizer splits a text into \"tokens\" (which are like words, but can be sub-word pieces, as you see below):"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:40:00.194678Z","iopub.status.busy":"2023-10-02T09:40:00.194097Z","iopub.status.idle":"2023-10-02T09:40:00.208470Z","shell.execute_reply":"2023-10-02T09:40:00.207319Z","shell.execute_reply.started":"2023-10-02T09:40:00.194645Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['g',\n"," \"'\",\n"," 'day',\n"," 'folks',\n"," ',',\n"," 'welcome',\n"," 'to',\n"," 'co',\n"," '##s',\n"," '##30',\n"," '##01',\n"," '##8',\n"," 'intelligent',\n"," 'system']"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["tokz.tokenize(\"G'day folks, welcome to COS30018 Intelligent system\")"]},{"cell_type":"markdown","metadata":{},"source":["Uncommon words will be split into pieces. Tokens that are partial words is appended with `##`."]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:40:00.210703Z","iopub.status.busy":"2023-10-02T09:40:00.209884Z","iopub.status.idle":"2023-10-02T09:40:00.218106Z","shell.execute_reply":"2023-10-02T09:40:00.217070Z","shell.execute_reply.started":"2023-10-02T09:40:00.210669Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['a',\n"," 'pl',\n"," '##at',\n"," '##yp',\n"," '##us',\n"," 'is',\n"," 'an',\n"," 'or',\n"," '##ni',\n"," '##thor',\n"," '##hy',\n"," '##nch',\n"," '##us',\n"," 'ana',\n"," '##tin',\n"," '##us',\n"," '.']"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["tokz.tokenize(\"A platypus is an ornithorhynchus anatinus.\")"]},{"cell_type":"markdown","metadata":{},"source":["Here's a simple function which tokenizes our `Text` input. Note that since the maximum token length is only 512, we need to truncate some values."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:23:54.689005Z","iopub.status.busy":"2023-10-02T10:23:54.688647Z","iopub.status.idle":"2023-10-02T10:23:54.694189Z","shell.execute_reply":"2023-10-02T10:23:54.693164Z","shell.execute_reply.started":"2023-10-02T10:23:54.688975Z"},"trusted":true},"outputs":[],"source":["def tok_func(x): return tokz(x[\"Text\"], truncation=True, max_length=max_length)"]},{"cell_type":"markdown","metadata":{},"source":["To run this quickly in parallel on every row in our dataset, use `map`"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:23:56.958385Z","iopub.status.busy":"2023-10-02T10:23:56.958054Z","iopub.status.idle":"2023-10-02T10:24:02.475164Z","shell.execute_reply":"2023-10-02T10:24:02.474218Z","shell.execute_reply.started":"2023-10-02T10:23:56.958358Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b6325bc441a49f88fec45a2cb710f95","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/4556 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b58f4680f5864b519b88af25a518e032","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/569 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"019b39e30b3049b3af45927063072ca4","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/570 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tok_ds = ds.map(tok_func, batched=True)"]},{"cell_type":"markdown","metadata":{},"source":["This adds a new item to our dataset called `input_ids` along with 2 additional items `token_type_ids, 'attention_mask'. For instance, here is the input and IDs for the second row of our data:"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:24:02.477804Z","iopub.status.busy":"2023-10-02T10:24:02.476908Z","iopub.status.idle":"2023-10-02T10:24:02.488584Z","shell.execute_reply":"2023-10-02T10:24:02.487683Z","shell.execute_reply.started":"2023-10-02T10:24:02.477770Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('Subject: vince and stinson ,  i got this resume from my friend ming sit who has a ph . d . from stanford .  please take a look at his resume to see if we can use him . i classify him as  a structurer , but things may change after all these years .  zimin  - - - - - - - - - - - - - - - - - - - - - - forwarded by zimin lu / hou / ect on 05 / 17 / 2000 04 : 08 pm  - - - - - - - - - - - - - - - - - - - - - - - - - - -  \" sit , ming \" on 05 / 17 / 2000 02 : 41 : 50 pm  to : \" zimin lu ( e - mail ) \"  cc :  subject :  - resume . doc',\n"," [101,\n","  3395,\n","  1024,\n","  12159,\n","  1998,\n","  2358,\n","  7076,\n","  2239,\n","  1010,\n","  1045,\n","  2288,\n","  2023,\n","  13746,\n","  2013,\n","  2026,\n","  2767,\n","  11861,\n","  4133,\n","  2040,\n","  2038,\n","  1037,\n","  6887,\n","  1012,\n","  1040,\n","  1012,\n","  2013,\n","  8422,\n","  1012,\n","  3531,\n","  2202,\n","  1037,\n","  2298,\n","  2012,\n","  2010,\n","  13746,\n","  2000,\n","  2156,\n","  2065,\n","  2057,\n","  2064,\n","  2224,\n","  2032,\n","  1012,\n","  1045,\n","  26268,\n","  2032,\n","  2004,\n","  1037,\n","  3252,\n","  2099,\n","  1010,\n","  2021,\n","  2477,\n","  2089,\n","  2689,\n","  2044,\n","  2035,\n","  2122,\n","  2086,\n","  1012,\n","  1062,\n","  27605,\n","  2078,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  2830,\n","  2098,\n","  2011,\n","  1062,\n","  27605,\n","  2078,\n","  11320,\n","  1013,\n","  7570,\n","  2226,\n","  1013,\n","  14925,\n","  2102,\n","  2006,\n","  5709,\n","  1013,\n","  2459,\n","  1013,\n","  2456,\n","  5840,\n","  1024,\n","  5511,\n","  7610,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1011,\n","  1000,\n","  4133,\n","  1010,\n","  11861,\n","  1000,\n","  2006,\n","  5709,\n","  1013,\n","  2459,\n","  1013,\n","  2456,\n","  6185,\n","  1024,\n","  4601,\n","  1024,\n","  2753,\n","  7610,\n","  2000,\n","  1024,\n","  1000,\n","  1062,\n","  27605,\n","  2078,\n","  11320,\n","  1006,\n","  1041,\n","  1011,\n","  5653,\n","  1007,\n","  1000,\n","  10507,\n","  1024,\n","  3395,\n","  1024,\n","  1011,\n","  13746,\n","  1012,\n","  9986,\n","  102])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["row = tok_ds[\"train\"][1]\n","row['Text'], row['input_ids']"]},{"cell_type":"markdown","metadata":{},"source":["The token IDs comes from a list called vocab in the tokenizer which contains a unique integer for every possible token string. We can look them up like this, for instance to find the token for the first word \"this\":"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:40:06.617499Z","iopub.status.busy":"2023-10-02T09:40:06.616815Z","iopub.status.idle":"2023-10-02T09:40:06.660612Z","shell.execute_reply":"2023-10-02T09:40:06.659132Z","shell.execute_reply.started":"2023-10-02T09:40:06.617454Z"},"trusted":true},"outputs":[{"data":{"text/plain":["2023"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["tokz.vocab['this']"]},{"cell_type":"markdown","metadata":{},"source":["Looking above at our input IDs, we do indeed see that `2023` appears as expected.\n","\n","Finally, we need to prepare our labels. Transformers always assumes that your labels has the column name `labels`, but in our dataset it's currently `Spam`. Therefore, we need to rename it. Then we need to remove redundant columns e.g. `Text`."]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:40:06.662962Z","iopub.status.busy":"2023-10-02T09:40:06.662322Z","iopub.status.idle":"2023-10-02T09:40:06.686705Z","shell.execute_reply":"2023-10-02T09:40:06.685514Z","shell.execute_reply.started":"2023-10-02T09:40:06.662926Z"},"trusted":true},"outputs":[],"source":["tok_ds = tok_ds.rename_columns({'Spam':'labels'})\n","tok_ds = tok_ds.remove_columns(['Text'])"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:40:06.689241Z","iopub.status.busy":"2023-10-02T09:40:06.688406Z","iopub.status.idle":"2023-10-02T09:40:06.696274Z","shell.execute_reply":"2023-10-02T09:40:06.695128Z","shell.execute_reply.started":"2023-10-02T09:40:06.689196Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 4556\n","    })\n","    val: Dataset({\n","        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 569\n","    })\n","    test: Dataset({\n","        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 570\n","    })\n","})"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["tok_ds"]},{"cell_type":"markdown","metadata":{},"source":["## Finetuning"]},{"cell_type":"markdown","metadata":{},"source":["To train a model in Transformers we'll use the `Trainer` class which has already implemented the training loop and other things for us."]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:40:06.698455Z","iopub.status.busy":"2023-10-02T09:40:06.697786Z","iopub.status.idle":"2023-10-02T09:40:17.863985Z","shell.execute_reply":"2023-10-02T09:40:17.862875Z","shell.execute_reply.started":"2023-10-02T09:40:06.698421Z"},"trusted":true},"outputs":[],"source":["from transformers import TrainingArguments,Trainer"]},{"cell_type":"markdown","metadata":{},"source":["We pick a batch size that fits our GPU, and small number of epochs so we can run experiments quickly. We also define the learning rate for the model."]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:40:17.865958Z","iopub.status.busy":"2023-10-02T09:40:17.865614Z","iopub.status.idle":"2023-10-02T09:40:17.871899Z","shell.execute_reply":"2023-10-02T09:40:17.870068Z","shell.execute_reply.started":"2023-10-02T09:40:17.865924Z"},"trusted":true},"outputs":[],"source":["bs = 16\n","epochs = 3\n","lr = 8e-5"]},{"cell_type":"markdown","metadata":{},"source":["Transformers uses the `TrainingArguments class` to set up arguments. Don't worry too much about the values we're using here, they should generally work fine in most cases. It's just the 3 parameters above that you may need to change for different models."]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:40:17.874080Z","iopub.status.busy":"2023-10-02T09:40:17.873438Z","iopub.status.idle":"2023-10-02T09:40:17.924129Z","shell.execute_reply":"2023-10-02T09:40:17.923094Z","shell.execute_reply.started":"2023-10-02T09:40:17.874028Z"},"trusted":true},"outputs":[],"source":["args = TrainingArguments(\n","    'email-spam-classification',\n","    learning_rate=lr,\n","    warmup_ratio=0.1,\n","    lr_scheduler_type='linear',\n","    fp16=True,\n","    evaluation_strategy=\"steps\",\n","    per_device_train_batch_size=bs,\n","    per_device_eval_batch_size=bs,\n","    num_train_epochs=epochs,\n","    logging_steps=100,\n","    eval_steps=100,\n","    weight_decay=0.01,\n","    report_to='none',\n","    push_to_hub=True,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["The pretrained model can be loaded using `AutoModelForSequenceClassification`"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:40:17.926403Z","iopub.status.busy":"2023-10-02T09:40:17.925740Z","iopub.status.idle":"2023-10-02T09:40:17.935731Z","shell.execute_reply":"2023-10-02T09:40:17.934697Z","shell.execute_reply.started":"2023-10-02T09:40:17.926366Z"},"trusted":true},"outputs":[],"source":["id2label = {0: \"Ham\", 1: \"Spam\"}\n","label2id = {\"Ham\": 0, \"Spam\": 1}"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:40:17.937865Z","iopub.status.busy":"2023-10-02T09:40:17.937460Z","iopub.status.idle":"2023-10-02T09:40:22.080743Z","shell.execute_reply":"2023-10-02T09:40:22.079692Z","shell.execute_reply.started":"2023-10-02T09:40:17.937823Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"29fdd89f840841c4b5b32c65ef836a64","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = AutoModelForSequenceClassification.from_pretrained(\n","    model_nm,\n","    num_labels=2,\n","    id2label=id2label,\n","    label2id=label2id,\n","    return_dict=True,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Let's check the number of trainable parameter for this model."]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:40:22.083157Z","iopub.status.busy":"2023-10-02T09:40:22.082198Z","iopub.status.idle":"2023-10-02T09:40:22.089634Z","shell.execute_reply":"2023-10-02T09:40:22.088239Z","shell.execute_reply.started":"2023-10-02T09:40:22.083122Z"},"trusted":true},"outputs":[],"source":["def print_trainable_parameters(model):\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:40:22.097200Z","iopub.status.busy":"2023-10-02T09:40:22.096835Z","iopub.status.idle":"2023-10-02T09:40:22.109177Z","shell.execute_reply":"2023-10-02T09:40:22.108128Z","shell.execute_reply.started":"2023-10-02T09:40:22.097163Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 109483778 || all params: 109483778 || trainable%: 100.0\n"]}],"source":["print_trainable_parameters(model)"]},{"cell_type":"markdown","metadata":{},"source":["109,483,778 parameters is a pretty large number and it could takes a very long time to finetune our model. Instead of a full finetuning, we will apply the IA3 method to significantly optimize the number of trainable parameters."]},{"cell_type":"markdown","metadata":{},"source":["First, we can view the overall architechture of the model by just print it."]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:40:22.111705Z","iopub.status.busy":"2023-10-02T09:40:22.111023Z","iopub.status.idle":"2023-10-02T09:40:22.125201Z","shell.execute_reply":"2023-10-02T09:40:22.124220Z","shell.execute_reply.started":"2023-10-02T09:40:22.111670Z"},"trusted":true},"outputs":[{"data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://docs.adapterhub.ml/_images/ia3.png\" width=\"300\" height=\"500\" />\n","<div>\n","    IA3 architechture from <a href=\"https://docs.adapterhub.ml/methods.html\">adapterhub.ml</a>\n","<div>"]},{"cell_type":"markdown","metadata":{},"source":["Base on the `BERT` and `IA3` architechture, we will inject trainable vectors into 3 components: `query`, `value` and `output.dense`. IA3 method is fully supported by the `peft` library. We can define the config and load of IA3 model using `PeftModelForSequenceClassification`."]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:40:22.127607Z","iopub.status.busy":"2023-10-02T09:40:22.126621Z","iopub.status.idle":"2023-10-02T09:40:22.898483Z","shell.execute_reply":"2023-10-02T09:40:22.897157Z","shell.execute_reply.started":"2023-10-02T09:40:22.127572Z"},"trusted":true},"outputs":[],"source":["from peft import PeftModelForSequenceClassification, get_peft_config\n","\n","config = {\n","    \"peft_type\": \"IA3\",\n","    \"task_type\": \"SEQ_CLS\",\n","    \"inference_mode\": False,\n","    \"target_modules\": [\"query\", \"value\", \"output.dense\"],\n","    \"feedforward_modules\": [\"output.dense\"],\n","    \"modules_to_save\": [\"classifier\"]\n","}\n","\n","peft_config = get_peft_config(config)\n","model = PeftModelForSequenceClassification(model, peft_config)"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:40:22.900888Z","iopub.status.busy":"2023-10-02T09:40:22.900231Z","iopub.status.idle":"2023-10-02T09:40:22.908608Z","shell.execute_reply":"2023-10-02T09:40:22.907517Z","shell.execute_reply.started":"2023-10-02T09:40:22.900841Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 67,588 || all params: 109,549,828 || trainable%: 0.061696126076984804\n"]}],"source":["model.print_trainable_parameters()"]},{"cell_type":"markdown","metadata":{},"source":["The number of trainable parameter has been reduce to only 67,588 which is **0.06%** of the pretrained model!"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:40:22.911251Z","iopub.status.busy":"2023-10-02T09:40:22.910182Z","iopub.status.idle":"2023-10-02T09:40:22.956192Z","shell.execute_reply":"2023-10-02T09:40:22.955146Z","shell.execute_reply.started":"2023-10-02T09:40:22.911214Z"},"trusted":true},"outputs":[{"data":{"text/plain":["PeftModelForSequenceClassification(\n","  (base_model): IA3Model(\n","    (model): BertForSequenceClassification(\n","      (bert): BertModel(\n","        (embeddings): BertEmbeddings(\n","          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","          (position_embeddings): Embedding(512, 768)\n","          (token_type_embeddings): Embedding(2, 768)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (encoder): BertEncoder(\n","          (layer): ModuleList(\n","            (0-11): 12 x BertLayer(\n","              (attention): BertAttention(\n","                (self): BertSelfAttention(\n","                  (query): Linear(\n","                    in_features=768, out_features=768, bias=True\n","                    (ia3_l): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 768x1])\n","                  )\n","                  (key): Linear(in_features=768, out_features=768, bias=True)\n","                  (value): Linear(\n","                    in_features=768, out_features=768, bias=True\n","                    (ia3_l): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 768x1])\n","                  )\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","                (output): BertSelfOutput(\n","                  (dense): Linear(\n","                    in_features=768, out_features=768, bias=True\n","                    (ia3_l): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 1x768])\n","                  )\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (intermediate): BertIntermediate(\n","                (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                (intermediate_act_fn): GELUActivation()\n","              )\n","              (output): BertOutput(\n","                (dense): Linear(\n","                  in_features=3072, out_features=768, bias=True\n","                  (ia3_l): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 1x3072])\n","                )\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","          )\n","        )\n","        (pooler): BertPooler(\n","          (dense): Linear(in_features=768, out_features=768, bias=True)\n","          (activation): Tanh()\n","        )\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","      (classifier): ModulesToSaveWrapper(\n","        (original_module): Linear(in_features=768, out_features=2, bias=True)\n","        (modules_to_save): ModuleDict(\n","          (default): Linear(in_features=768, out_features=2, bias=True)\n","        )\n","      )\n","    )\n","  )\n",")"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"markdown","metadata":{},"source":["We can inspect the PEFT model to verify that the trainable vectors have been injected into the correct components (look for `ia3_l`)."]},{"cell_type":"markdown","metadata":{},"source":["Before actually train the model, we need to define 2 additionals things: a data collator and a compute_metrics function."]},{"cell_type":"markdown","metadata":{},"source":["The data collator will group multiple inputs into batches. Then it will add a special tokens call \"pad token\" to make all items in a batch equal in length. That process effectively forms matrices. By doing that, we can take advantages of GPU parallel processing power since it is designed to efficiently perform operations on matrices."]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:40:22.958969Z","iopub.status.busy":"2023-10-02T09:40:22.958131Z","iopub.status.idle":"2023-10-02T09:40:22.967717Z","shell.execute_reply":"2023-10-02T09:40:22.966698Z","shell.execute_reply.started":"2023-10-02T09:40:22.958921Z"},"trusted":true},"outputs":[],"source":["from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokz)"]},{"cell_type":"markdown","metadata":{},"source":["The compute_metrics function, as its name suggests, output the metric value for model evaluation. For this demo, we will use the accuracy metrics. The accuracy of a model shows how many predictions are correct over all predictions."]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:40:22.970296Z","iopub.status.busy":"2023-10-02T09:40:22.969642Z","iopub.status.idle":"2023-10-02T09:40:25.063660Z","shell.execute_reply":"2023-10-02T09:40:25.062515Z","shell.execute_reply.started":"2023-10-02T09:40:22.970262Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"567a191daeb84d7eac21003c06610051","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import evaluate\n","import numpy as np\n","\n","accuracy = evaluate.load(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return accuracy.compute(predictions=predictions, references=labels)"]},{"cell_type":"markdown","metadata":{},"source":["Now we can finally create the `Trainer` and train our model."]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:40:25.066127Z","iopub.status.busy":"2023-10-02T09:40:25.065362Z","iopub.status.idle":"2023-10-02T09:40:31.664860Z","shell.execute_reply":"2023-10-02T09:40:31.663814Z","shell.execute_reply.started":"2023-10-02T09:40:25.066088Z"},"trusted":true},"outputs":[],"source":["trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=tok_ds['train'],\n","    eval_dataset=tok_ds['val'],\n","    tokenizer=tokz,\n","    compute_metrics=compute_metrics,\n","    data_collator=data_collator\n",")"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:40:31.666431Z","iopub.status.busy":"2023-10-02T09:40:31.666054Z","iopub.status.idle":"2023-10-02T09:52:04.340080Z","shell.execute_reply":"2023-10-02T09:52:04.339007Z","shell.execute_reply.started":"2023-10-02T09:40:31.666397Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='855' max='855' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [855/855 11:29, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.599600</td>\n","      <td>0.513706</td>\n","      <td>0.759227</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.468200</td>\n","      <td>0.499727</td>\n","      <td>0.762742</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.467600</td>\n","      <td>0.445968</td>\n","      <td>0.789104</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.396300</td>\n","      <td>0.429654</td>\n","      <td>0.803163</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.402400</td>\n","      <td>0.403716</td>\n","      <td>0.822496</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.395500</td>\n","      <td>0.390500</td>\n","      <td>0.829525</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.365900</td>\n","      <td>0.395892</td>\n","      <td>0.827768</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.360500</td>\n","      <td>0.392036</td>\n","      <td>0.831283</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=855, training_loss=0.42854927464535364, metrics={'train_runtime': 692.3448, 'train_samples_per_second': 19.742, 'train_steps_per_second': 1.235, 'total_flos': 3586829861952000.0, 'train_loss': 0.42854927464535364, 'epoch': 3.0})"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["The training result shows that both training and validation loss decrease. This means we have successfully trained our model. Let's evaluate the final model on the validation set."]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:52:04.342535Z","iopub.status.busy":"2023-10-02T09:52:04.341519Z","iopub.status.idle":"2023-10-02T09:52:15.828391Z","shell.execute_reply":"2023-10-02T09:52:15.827212Z","shell.execute_reply.started":"2023-10-02T09:52:04.342499Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [36/36 00:11]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_loss': 0.38854390382766724,\n"," 'eval_accuracy': 0.8330404217926186,\n"," 'eval_runtime': 11.4759,\n"," 'eval_samples_per_second': 49.582,\n"," 'eval_steps_per_second': 3.137,\n"," 'epoch': 3.0}"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["trainer.evaluate()"]},{"cell_type":"markdown","metadata":{},"source":["We achieve around 80% accuracy on the validation set. Not too bad given that we only train it for just 3 epoches."]},{"cell_type":"markdown","metadata":{},"source":["After the training process, the IA3 vectors can be merged back into the pretrained model for faster inference."]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:52:15.831082Z","iopub.status.busy":"2023-10-02T09:52:15.829918Z","iopub.status.idle":"2023-10-02T09:52:16.211578Z","shell.execute_reply":"2023-10-02T09:52:16.210586Z","shell.execute_reply.started":"2023-10-02T09:52:15.831020Z"},"trusted":true},"outputs":[],"source":["merged_model = trainer.model.merge_and_unload()"]},{"cell_type":"markdown","metadata":{},"source":["**Optional:** Push our finetuned model to huggingface hub for future use"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:52:16.213127Z","iopub.status.busy":"2023-10-02T09:52:16.212765Z","iopub.status.idle":"2023-10-02T09:52:16.943478Z","shell.execute_reply":"2023-10-02T09:52:16.942358Z","shell.execute_reply.started":"2023-10-02T09:52:16.213093Z"},"trusted":true},"outputs":[{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/legacy107/email-spam-classification-merged/commit/c91c506ad5e621e354a26f4b261a643dd3673b46', commit_message='Upload tokenizer', commit_description='', oid='c91c506ad5e621e354a26f4b261a643dd3673b46', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["trainer.tokenizer.push_to_hub(\"legacy107/email-spam-classification-merged\")"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:52:16.945829Z","iopub.status.busy":"2023-10-02T09:52:16.945171Z","iopub.status.idle":"2023-10-02T09:52:28.962345Z","shell.execute_reply":"2023-10-02T09:52:28.961277Z","shell.execute_reply.started":"2023-10-02T09:52:16.945791Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"46122dceb5614c6e8ed46c59a88abfdd","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/legacy107/email-spam-classification-merged/commit/3b18ac0ea02d7dd95b8b7856c00491dfdfd3b3bf', commit_message='Upload BertForSequenceClassification', commit_description='', oid='3b18ac0ea02d7dd95b8b7856c00491dfdfd3b3bf', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["merged_model.push_to_hub(\"legacy107/email-spam-classification-merged\")"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluate"]},{"cell_type":"markdown","metadata":{},"source":["**Optional**: Load model from huggingface hub"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:27:34.004363Z","iopub.status.busy":"2023-10-02T10:27:34.003898Z","iopub.status.idle":"2023-10-02T10:27:35.564043Z","shell.execute_reply":"2023-10-02T10:27:35.563023Z","shell.execute_reply.started":"2023-10-02T10:27:34.004332Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification,AutoTokenizer\n","merged_model = AutoModelForSequenceClassification.from_pretrained(\"legacy107/email-spam-classification-merged\")\n","tokz = AutoTokenizer.from_pretrained(model_nm, model_max_length=max_length)"]},{"cell_type":"markdown","metadata":{},"source":["To evaluate our model on the test set, we will use the `evaluate` library from huggingface. First, initialise an evaluator for text classification. Then pass the model, the test data, the metric and other required parameters to the evaluator."]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T09:52:28.964857Z","iopub.status.busy":"2023-10-02T09:52:28.963854Z","iopub.status.idle":"2023-10-02T09:52:39.068668Z","shell.execute_reply":"2023-10-02T09:52:39.067660Z","shell.execute_reply.started":"2023-10-02T09:52:28.964819Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'accuracy': 0.8631578947368421,\n"," 'total_time_in_seconds': 9.84486404900008,\n"," 'samples_per_second': 57.898209377293895,\n"," 'latency_in_seconds': 0.017271691314035227}"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["from evaluate import evaluator\n","from datasets import load_dataset\n","\n","task_evaluator = evaluator(\"text-classification\")\n","results = task_evaluator.compute(\n","    model_or_pipeline=merged_model,\n","    data=ds[\"test\"],\n","    input_column=\"Text\",\n","    label_column=\"Spam\",\n","    metric=\"accuracy\",\n","    label_mapping=label2id,\n","    strategy=\"simple\",\n","    tokenizer = tokz,\n",")\n","results"]},{"cell_type":"markdown","metadata":{},"source":["An accuracy of 86.3% indicate that our finetuned model is can correctly classify 86.3% of the total emails in the test set. "]},{"cell_type":"markdown","metadata":{},"source":["## Inference"]},{"cell_type":"markdown","metadata":{},"source":["Now we can use our model to run inference on actual data using the text classification pipeline from `transformers` library"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:34:28.141462Z","iopub.status.busy":"2023-10-02T10:34:28.140925Z","iopub.status.idle":"2023-10-02T10:34:28.815731Z","shell.execute_reply":"2023-10-02T10:34:28.814764Z","shell.execute_reply.started":"2023-10-02T10:34:28.141422Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Text: Subject: look 10 years younger - free sample ! ! ! ! ! ! ! ! esoy  this e - mail ad is being sent in full compliance with u . s . senate bill 1618 , title # 3 , section 301  to remove yourself send a blank e - mail to : removal 992002 @ yahoo . com  free sample ! free tape !  new cosmetic breakthru !  look 10 years younger in ( 6 ) weeks or less !  look good duo . . from the inside out . . . . .  > from the outside in !  introducing . . . . natures answer to faster  and more obvious results for :  * * wrinkles  * * cellulite  * * dark circles  * * brown spots . . .  * * lifts the skin  * * strenghtens the hair and nails  also helps to . . . . . . . .  * reduce cell damage from excessive sun exposure  * stimulate colllagen formation  * provide protection against skin disorder  * and is hopoallergenic  find out what ! where ! and how !  to order your free sample and tape send your  request to :  lookyoungnow 2000 @ yahoo . com  subject : subscribe to free sample :  your name : . . . . . . . . . . . . . . . . . . .  street address : . . . . . . . . . . . . . .  city : . . . . . . . . . . . . . . . . . . . . . . . .  state and zip code : . . . . . . . . . .  email address : . . . . . . . . . . . . . . .  sxjrohvneydjgucyfa\n","Label: 1\n","Prediction: Spam\n"]}],"source":["from transformers import pipeline\n","\n","row_id = 10\n","text = ds[\"test\"][row_id][\"Text\"]\n","classifier = pipeline(\"text-classification\", model=merged_model, tokenizer=tokz)\n","prediction = classifier(text)\n","\n","print(f'Text: {ds[\"test\"][row_id][\"Text\"]}')\n","print(f'Label: {ds[\"test\"][row_id][\"Spam\"]}')\n","print(f'Prediction: {prediction[0][\"label\"]}')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
